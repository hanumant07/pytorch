{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from vae.papers.feature_consistent import Trainer, VAEModel\n",
    "from datasets.celeb_a import CelebADataset\n",
    "from utils import ExperimentLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 64\n",
    "NUM_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=3\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms() -> transforms.Compose:\n",
    "  \"\"\" Returns transforms associated with celebA related papers\n",
    "\n",
    "  Applies, resize->center crop->normalization\n",
    "  \"\"\"\n",
    "  return transforms.Compose([\n",
    "      transforms.Resize(IMAGE_SIZE),\n",
    "      transforms.CenterCrop(IMAGE_SIZE),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "  ])\n",
    "\n",
    "\n",
    "def construct_hyperparams() -> Dict:\n",
    "  \"\"\" Constructs dictionary of hyperparameters\n",
    "\n",
    "  Hyperparameters defined in arxiv.org/pdf/1610.00291.pdf\n",
    "  \"\"\"\n",
    "  return {\n",
    "      'lr': 0.005,\n",
    "      'num_epochs': NUM_EPOCHS,\n",
    "      'batch_size': BATCH_SIZE,\n",
    "      'gamma': 0.5,\n",
    "      'loss_alpha': 1,\n",
    "      'loss_beta': 0.5\n",
    "  }\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "  if torch.cuda.is_available():\n",
    "    return torch.device('cuda')\n",
    "  else:\n",
    "    return torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders() -> Tuple[DataLoader, DataLoader]:\n",
    "  celeba_train_ds = CelebADataset(root=\"../data/celeba\",\n",
    "                                  train=True,\n",
    "                                  split=0.3,\n",
    "                                  transforms=get_transforms())\n",
    "  celeba_test_ds = CelebADataset(root=\"../data/celeba\",\n",
    "                                 split=0.3,\n",
    "                                 transforms=get_transforms())\n",
    "  celeba_train_dl = DataLoader(celeba_train_ds,\n",
    "                               shuffle=True,\n",
    "                               batch_size=BATCH_SIZE)\n",
    "  celeba_test_dl = DataLoader(celeba_test_ds, batch_size=BATCH_SIZE)\n",
    "  return (celeba_train_dl, celeba_train_ds, celeba_test_dl, celeba_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAEModel().to(device)\n",
    "device = get_device()\n",
    "logger = ExperimentLogger(\"../logs\", 'celeba_vae')\n",
    "model.to(device)\n",
    "trainer = Trainer(model=model,\n",
    "                    vgg_variant='123',\n",
    "                    device=device,\n",
    "                    hyper_params=construct_hyperparams())\n",
    "checkpoint_path = \"../models/var/perceptual_loss_var.tar\"\n",
    "train_dl, train_ds, test_dl, test_ds = get_loaders()\n",
    "\n",
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = trainer.load_state(model, checkpoint_path)\n",
    "epoch_train_losses.append(train_dict['train_loss'])\n",
    "epoch_test_losses.append(train_dict['test_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  epoch_train_loss, train_loss_list = trainer.run_train_epoch(model, train_dl, train_ds)\n",
    "  epoch_test_loss, test_loss_list = trainer.run_test_loop(model, test_dl, test_ds)\n",
    "  epoch_train_losses.append(epoch_train_loss)\n",
    "  epoch_test_losses.append(epoch_test_loss)\n",
    "  print(f'epoch {epoch+1} train_loss = {epoch_train_loss:.4f}, test_loss = {epoch_test_loss:.4f}')\n",
    "  logger.save_loss(train_loss_list, test_loss_list, epoch)\n",
    "logger.save_loss(epoch_train_losses, epoch_test_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state(model, NUM_EPOCHS, \"../models/var/perceptual_loss_var.tar\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2486d21a85745cbeb8f10f3d9a2d0f97bc7a2cb7a7632d175e2408d74999663e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
